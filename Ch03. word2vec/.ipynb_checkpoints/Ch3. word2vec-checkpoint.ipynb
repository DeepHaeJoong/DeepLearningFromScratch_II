{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch3. word2vec\n",
    "\n",
    "### Recap [Ch 2. 단어의 분산 표현]\n",
    "\n",
    "- 통계 기반 기법 : 단어의 빈도를 가지고 표현\n",
    "- **추론 기반 기법** : 이번 장에서 다루는 기법들.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이번 장에서 배울 내용\n",
    "\n",
    "- 추론 기반 기법은 추측하는 것이 목적이며, 그 부산물로 단어의 분산 표현을 얻을 수 있다.\n",
    "- word2vec은 추론 기반 기법이며, 단순한 2층 신경망이다.\n",
    "- word2vec은 skip-gram 모델과 CBOW 모델을 제공한다.\n",
    "- CBOW 모델은 여러 단어(맥락)로부터 하나의 단어(타깃)를 추측한다.\n",
    "- 반대로 skip-gram 모델은 하나의 단어(타깃)로부터 다수의 단어(맥락)을 추측한다.\n",
    "- word2vec은 가중치를 다시 학습할 수 있으므로, 단어의 분산 표현 갱신이나 새로운 단어 추가를 효율적으로 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론 기반 기법이란?\n",
    "   - 추론 하는 기법\n",
    "   - 대표적으로 신경망을 이용한 **word2vec** 이 이에 해당한다.\n",
    "\n",
    "### 이번장에서 다룰 내용\n",
    "\n",
    "- word2vec의 구조\n",
    "- '단순한' word2vec 구현\n",
    "    - Ch4에서 '진짜' word2vec을 완성할 예정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 추론 기반 기법과 신경망\n",
    "\n",
    "단어를 벡터로 표현하는 방법은 지금까지 활발히 연구되었으며, 그중에서도 성공적인 기법들은 크게 두 부류로 나눌 수 있다.\n",
    "\n",
    "- **통계 기반 기법**\n",
    "- **추론 기반 기법**\n",
    "\n",
    "차이점 : 단어의 의미를 얻는 방식  \n",
    "공통점 : 분포 가설을 가짐 ?? 어떤 의미인가\n",
    "\n",
    " 이번 절에서는 통계 기반 기법의 문제를 지적하고, 그 대안인 추론 기반 기법의 이점을 거시적인 관점에서 설명한다. 그 이후 `word2vec`의 전처리를 위해 신경망으로 '단어'를 처리하는 예를 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 통계 기반 기법의 문제점\n",
    "\n",
    "#### 계산 복잡도\n",
    "\n",
    "통계 기반 기법은 주변 단어의 빈도를 기초로 단어를 표현했다. 구체적으로는 단어의 동시발생 행렬을 만들고 그 행렬을 만들고 그 행렬에 **SVD**를 적용하여 밀집벡터(단어의 분산 표현)를 얻었다. 그러나 이 방식은 대규모 말뭉치를 다룰 때 문제가 발생한다.\n",
    "\n",
    "현실에서 다루는 말뭉치의 어휘 수는 무수히 많다. 예를 들어 영어의 어휘 수는 100만을 넘는다고 하는데, 통계 기반 기법에서는 형상이 `100만 x 100만`인 행렬을 만들게 된다. 이런 거대 행렬에 **SVD** 기법을 적용하는 것은 현실적이지 않다.\n",
    "\n",
    "> **NOTE**  \n",
    "> SVD를 $n \\times n$ 행렬에 적용하는 비용은 $Q(n^{n})$이다. 슈퍼컴퓨터로 동원해도 처리할 수 없는 수준이라고 한다. 근사적인 기법?과 희소행렬의 성질 등을 이용해 속도를 개선할 수 있다고 한다. 하지만 여전히 상당한 컴퓨팅 자원을 들여 장시간 계산해야 한다.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-1.png?raw=False\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 추론 기반 기법 개요\n",
    "\n",
    "**[그림 3-2]** 처럼 주변 단어(맥락)가 주어졌을 때 \"?\"에 무슨 단어가 들어가는지 추측하는 작업\n",
    "\n",
    "**[그림 3-2]** 주변 단어들을 맥락으로 사용해 \"?\"에 들어갈 단어를 추측한다.\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-2.png?raw=true\" width=\"700\">\n",
    "\n",
    "\n",
    "위처럼 추론 문제를 풀고 학습하는 것이 '추론 기반 기법'이 다루는 문제이다. 이러한 추론 문제를 계속해서 반복하면서 단어의 출현 패턴을 학습하는 것이다. '모델 관점'에서 보면, 이 추론 문제는 **[그림 3-3]**처럼 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-3]** 추론 기반 기법 : 맥락을 입력하면 모델은 각 단어의 출현 확률을 출력한다.\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-3.png?raw=true\" width=\"700\">\n",
    "\n",
    "위 처럼 추론 기반 기법에는 어떠한 모델이 등장한다. 이 모델로 신경망을 사용하며, 모델은 맥락 정보를 입력받아 (출현할 수 있는) 각 단어의 출현 확률을 출력한다. \n",
    "\n",
    "> **NOTE**  \n",
    "> 추론 기반 기법도 통계 기반 기법처럼 분포 가설에 기초하게 된다. **분포 가설이란?** **\"단어의 의미는 주변 단어에 의해 형성된다.\"**는 가설로, 추론 기반 기법에서는 이를 앞에서와 같은 추측 문제로 귀결시켰다. 이 처럼 두 기법 모두 분포 가설 근거하는 **'단어의 동시발생 가능성'**을 얼마나 잘 모델링 하는가 중요한 연구 주제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 신경망에서의 단어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 이용해 '단어'를 처리한다. 하지만, 신경망은 'you'와 'say' 등의 단어를 있는 그대로 처리할 수 없으니 단어를 '고정 길이의 벡터'로 변환해야 한다. 이때 사용하는 방법이 단어를 **원핫 표현(원핫 벡터)**으로 변환하는 것이다.\n",
    "\n",
    "**[그림 3-4]** : 단어, 단어 ID, 원핫 표현\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-4.png?raw=true\" width=\"700\">\n",
    "\n",
    "이를 통해 얻는 이점은 신경망의 입력층을 **[그림 3-5]**처럼 뉴런의 수를 **\"고정\"** 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-5]** : 입력층의 뉴런 : 각 뉴런이 각 단어에 대응(해당 뉴런이 1이면 파랑색, 0이면 회색)\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-5.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-6]** : 완전연결계층에 의한 변환 : 입력층의 각 뉴런은 7개의 단어 각각에 대응(은닉층 뉴런은 3개를 준비함)\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-6.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-7]** : 완전연결계층에 의한 변환을 단순화한 그림(완전연결계층의 가중치를 7 x 3 크기의 **W**라는 행렬로 표현)\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-7.png?raw=true\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36081838 0.93352609 1.61908657]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])  # 입력\n",
    "W = np.random.randn(7, 3)              # 가중치\n",
    "h = np.matmul(c, W)                    # 중간노드\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning**  \n",
    "> 이 코드에서는 입력 데이터(c)의 차원 수(ndim)은 2이다. 이는 미니배치 처리를 고려한 것으로 최초의 차원(0번째 차원)에 각 데이터를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndim(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-8]** : 맥락 **c**와 **W**의 곱으로 해당 위치의 행벡터가 추출된다. (각 요소의 가중치 크기는 흑백의 진하기로 표현)\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-8.png?raw=true\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.58576864 -1.50928081 -0.19590787]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "W = np.random.randn(7, 3)\n",
    "layer = MatMul(W)\n",
    "h = layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.2 단순한 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec 구현!\n",
    "\n",
    "**[그림 3-3]**의 '모델'을 신경망으로 구축해보자.\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-3.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 이번절에서는 **word2vec**에서의 **CBOW(continuous bag-of-words)** 모델이다.\n",
    " \n",
    " > **Warning**  \n",
    " > word2vec이라는 용어는 원래 프로그램이나 도구를 가리키는 데 사용됐습니다. 그런데 이 용어가 유명해지면서, 문맥에 따라서는 신경망 모델을 가리키는 경우도 많이 볼 수 있습니다. **CBOW** 모델과 **Skip-gram** 모델은 **word2vec**에서 사용되는 신경망입니다. 이번 절에서는 **CBOW** 모델을 중심으로 이야기를 풀어가며, 두 모델의 차이는 '3.5.2. skip-gram 모델'절에서 자세히 설명하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 CBOW 모델의 추론 처리\n",
    "\n",
    "맥락으로부터 **Target**을 추측하는 용도의 신경망이다. ('타깃'은 중앙 단어이고 그 주변 단어들이 '맥락'입니다.). 이 **CBOW** 모델이 가능한 한 정확하게 추론하도록 훈련시켜서 단어의 분산 표현을 얻어낼 것이다.\n",
    "\n",
    "**CBOW** 모델의 입력은 맥락입니다. \n",
    "- 맥락 : \"you\", \"goodbye\"\n",
    "- 타겟 : ?\n",
    "\n",
    "**[그림 3-9]** **CBOW**모델의 신경망 구조\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-9.png?raw=true\" width=\"500\">\n",
    "\n",
    "> **Warning**  \n",
    "> 이 그림에서 입력층이 2개인 이유는 맥락으로 고려할 단어를 2개로 정했기 때문이다. 즉, 맥락에 포함시킬 단어가 $N$개라면 입력층도 $N$가 된다.\n",
    "\n",
    "입력이 여러 개이면 전체를 **평균**하면 된다. 앞의 예에 대입해보면 다음과 같다. 완전연결계층에 의한 첫 번째 입력층이 $h_1$으로 변환되고, 두 번째 입력층이 $h_2$로 변환되었다고 하면, 은닉층 뉴런은 $\\frac{1}{2}(h_1 + h_2)$가 되는 것이다.\n",
    "\n",
    "마지막으로  **[그림 3-9]**의 출력층을 보면, 출력층의 뉴런은 총 7개인데, 여기서 중요한 것은 이 뉴런 하나하나가 각각의 단어에 대응한다는 것이다. 그리고 출력층 뉴런은 각 던어의 **'점수'**를 뜻하며, 값이 높을수록 대응 단어의 출현 확률도 높아진다. 여기서 점수란 확률로 해석되기 전의 값이고, 이 점수에 소프트맥스 함수를 적용해서 '확률'로 얻을 수 있다.\n",
    "\n",
    "> **Warning**  \n",
    "> 점수를 **Softmax** 계층에 통과시킨 후의 뉴런을 '출력층'이라고도 한다. 교재에서는 점수를 출력하는 노드를 '출력층'이라고 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-10]** 가중치의 각 행이 해당 단어의 분산 표현이다.\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-10.png?raw=true\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 $\\text{W}_{in}$의 각 행에는 해당 단어의 분산 표현이 담겨 있다고 볼 수 있다. 따라서, 학습을 진행할수록 맥락에서 출현하는 단어를 잘 추측하는 방향으로 이 분산 표현들이 업데이트 될 것이다. 학습을 할수록 놀랍게도 이렇게 해서 얻은 벡터에는 \"단어의 의미\"도 잘 녹아들어 있다! ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decoding : 인간이 이해할 수 있는 입력 데이터를 이해할 수 없는 은닉층의 정보로 바꾸어주는 작업\n",
    "- encoding : 인간이 이해할 수 없는 코드를 이해할 수 있는 정보로 표현해주는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-11]** 계층 관점에서 본 **CBOW** 모델의 신경망 구성 : **MatMul** 계층에서 사용하는 가중치 ($W_{in}$, $W_{out}$)는 해당 계층 안으로 넣었음 \n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-11.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7697392  -1.0003454   0.0701046  -1.93896422 -0.60562161  1.3181849\n",
      "  -0.17862606]]\n"
     ]
    }
   ],
   "source": [
    "# ch03/cbow_predict.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul\n",
    "\n",
    "# 샘플 맥락 데이터\n",
    "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "# 가중치 초기화\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "# 계층 생성\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 순전파\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer0.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치 초기화\n",
    "- 입력층 2개 생성\n",
    "- 출력 계층 1개 생성\n",
    "- $W_{in}$을 공유함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 CBOW 모델의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-12]** **CBOW** 모델의 구체적인 예(노드 값의 크기를 흑백의 진하기로 나타냄)\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-12.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 모델을 통해 출력층에서 각 단어의 점수(Score)를 출력했다. 이 점수에 소프트맥스 함수를 적용하면 \"확률\"을 얻을 수 있다. 이 확률은 맥락(전후 단어)이 주어졌을 때 그 가운데 어떤 단어가 출현하는지 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습시키는 데이터 셋에 따라서 단어의 분산 표현은 다를 수 밖에 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 학습은 기존 신경망을 학습시키는 방법과 똑같다. 즉, 소프트맥스와 교차 엔트로피 오차만 적용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-13]** **CBOW** 모델의 학습 시 신경망 구성\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-13.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교재에서는 위의 Softmax 계층과 CorssEntropy Error 계층을 Softmax with Loss라는 하나의 계층으로 구현함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-14]** Softmax 계층과 Cross Entropy Error 계층을 Softmax with Loss 계층으로 합침\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-14.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 word2vec의 가중치와 분산 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word2vec**에서 사용되는 신경망의 가중치는 다음과 같다\n",
    "\n",
    "- $W_{in}$ : 입력 ~ Layer\n",
    "- $W_{out}$ : Layer ~ 출력\n",
    "\n",
    "우선 입력 측 가중치 $W_{in}$가 각 행이 word2vec모델에서 사용되는 단어의 분산 표현에 해당한다.\n",
    "\n",
    "형상을 확인해보면서 비교해보자.\n",
    "\n",
    "**[그림 3-15]** 각 단어의 분산 표현은 입력 측과 출력 측 모두의 가중치에서 확인할 수 있다.\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-15.png?raw=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 형상을 보고 단어의 분산 표현으로는 어느 쪽 가중치를 선택하면 좋을까?\n",
    "\n",
    "- **A** : 입력 측의 가중치만 이용\n",
    "- **B** : 출력 측의 가중치만 이용\n",
    "- **C** : 입력 및 출력 모든 가중치 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word2vec**(특히 **skip-gram** 모델)에서는 **A** 안인 \"입력 측의 가중치만 이용한다.\"가 가장 대중적인 선택이다. 많은 연구에서도 그렇게 해왔으므로, 이를 따라서 $W_{in}$을 단어의 분산 표현으로 이용하겠다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 학습 데이터 준비\n",
    "\n",
    "\n",
    "> 말뭉치 : \"You say goodbye and I say hello.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 맥락과 타깃\n",
    "\n",
    "입력은 \"맥락\", 정답 레이블은 맥락에 둘러싸인 \"중안의 단어\", 즉, \"타깃\"  \n",
    "목표는 신경망에 \"맥락\"을 입력했을 때 \"타깃\"이 출현할 확률을 높이는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-16]** 말뭉치에서 맥락과 타깃을 만드는 예\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-16.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "말뭉치로부터 맥락과 타깃을 만드는 함수 구현 필요, 우선 말뭉치 텍스트를 단어 ID로 변환해야 한다.\n",
    "\n",
    "**함수 : `preprocess()` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.util import preprocess\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "\n",
    "print(word_to_id)\n",
    "\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런 다음 **\"단어 ID\"**의 배열인 corpus(문장)로부터 맥락과 타깃을 만들어야 한다.\n",
    "\n",
    "**[그림 3-17]** 단어 ID의 배열인 corpus로부터 맥락과 타깃을 작성하는 예 (맥락의 window 크기는 1)\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-17.png?raw=true\" width=\"600\">\n",
    "\n",
    "**함수 : `create_contexts_target()` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common.utill.py\n",
    "def create_contexts_target(corpus, window_size = 1):\n",
    "    target = corpus[window_size: -window_size]\n",
    "    contexts = []\n",
    "    \n",
    "    for idx in range(window_size, len(corpus) - window_size): # [1, 2, 3, 4, 5, 6]\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):  # [-1, 0, 1]\n",
    "            if t == 0: # target\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "        \n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size = 1)\n",
    "\n",
    "print(contexts)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것으로 말뭉치로부터 맥락과 타깃을 만들었다. 나중에 이를 **CBOW** 모델에 넘겨주면 된다. 하지만, 위의 맥락과 타깃을 보면 **단어 ID**로 출력된다.  \n",
    "이를 **One-Hot 표현**으로 변환이 필요하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 원핫 표현으로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-18]** \"맥락\"과 \"타깃\"을 원핫 표현으로 변환하는 예시\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-18.png?raw=true\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common.utill.py\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size = 1)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]]\n"
     ]
    }
   ],
   "source": [
    "print(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 CBOW 모델 구현\n",
    "\n",
    "**[그림 3-19]** CBOW 모델의 신경망 구성\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-14.png?raw=true\" width=\"600\">\n",
    "\n",
    "\n",
    "**함수 : `simpleCBOW()` ** 라는 이름으로 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch03/simple_cbow.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        #  인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 함수 : `forward()`**\n",
    "> 기능 : 인수로 맥락(contexts)과 타깃(target)을 받아 손실(loss)를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch03/simple_cbow.py\n",
    "def forward(self, contexts, target):\n",
    "    h0 = self.in_layer0.forward(contexts[:,0])\n",
    "    h1 = self.in_layer1.forward(contexts[:,1])\n",
    "    h = (h0 + h1) * 0.5\n",
    "    score = self.out_layer.forward(h)\n",
    "    loss = self.loss_layer.forward(score, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**contexts** : 3차원 Numpy array  (6, 2, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-20]** CBOW 모델의 역전파(역전파의 흐름은 두꺼운(붉은) 화살표로 표시)\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-20.png?raw=true\" width=\"800\">\n",
    "\n",
    "** 함수 : `backward()`**\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch03/simple_cbow.py\n",
    "def backward(self, dout=1):\n",
    "    ds = self.loss_layer.backward(dout)\n",
    "    da = self.out_layer.backward(ds)\n",
    "    da *= 0.5\n",
    "    self.in_layer1.backward(da)\n",
    "    self.in_layer0.backward(da)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 학습 코드 구현\n",
    "\n",
    "- 학습 데이터를 준비해 신경망에 입력한 다음, 기울기를 구하고 가중치 매개변수를 순서대로 갱신하는 코드. \n",
    "- 1장에서의 **Trainer 클래스**, **optimizer 클래스** 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 2 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 3 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 4 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 5 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 6 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 7 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 8 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 9 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 10 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 11 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 12 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 13 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 14 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 15 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 16 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 17 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 18 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 19 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 20 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 21 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 22 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 23 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 24 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 25 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 26 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 27 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 28 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 29 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 30 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 31 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 32 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 33 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 34 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 35 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 36 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 37 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 38 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 39 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 40 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 41 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 42 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 43 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 44 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 45 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 46 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 47 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 48 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 49 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 50 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 51 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 52 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 53 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 54 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 55 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 56 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 57 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 58 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 59 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 60 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 61 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 62 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 63 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 64 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 65 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 66 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 67 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 68 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 69 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 70 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 71 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 72 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 73 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 74 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 75 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 76 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 77 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 78 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 79 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 80 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 81 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 82 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 83 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 84 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 85 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 86 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
      "| 에폭 87 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 88 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
      "| 에폭 89 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
      "| 에폭 90 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 91 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 92 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
      "| 에폭 93 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
      "| 에폭 94 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
      "| 에폭 95 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 96 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
      "| 에폭 97 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 98 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
      "| 에폭 99 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
      "| 에폭 100 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 101 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 102 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 103 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 104 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 105 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
      "| 에폭 106 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
      "| 에폭 107 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
      "| 에폭 108 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
      "| 에폭 109 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
      "| 에폭 110 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 111 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
      "| 에폭 112 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
      "| 에폭 113 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
      "| 에폭 114 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
      "| 에폭 115 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 116 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
      "| 에폭 117 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
      "| 에폭 118 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
      "| 에폭 119 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 120 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
      "| 에폭 121 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
      "| 에폭 122 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 123 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
      "| 에폭 124 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
      "| 에폭 125 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
      "| 에폭 126 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 127 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
      "| 에폭 128 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 129 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
      "| 에폭 130 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
      "| 에폭 131 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 132 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
      "| 에폭 133 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
      "| 에폭 134 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
      "| 에폭 135 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 136 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
      "| 에폭 137 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 138 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
      "| 에폭 139 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
      "| 에폭 140 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
      "| 에폭 141 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 142 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 143 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 144 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 145 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 146 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 147 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 148 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
      "| 에폭 149 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
      "| 에폭 150 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
      "| 에폭 151 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
      "| 에폭 152 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 153 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
      "| 에폭 154 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 155 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
      "| 에폭 156 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
      "| 에폭 157 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 158 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
      "| 에폭 159 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 160 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 161 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 162 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 163 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 164 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
      "| 에폭 165 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
      "| 에폭 166 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 167 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
      "| 에폭 168 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 169 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 170 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 171 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
      "| 에폭 172 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
      "| 에폭 173 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 174 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
      "| 에폭 175 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
      "| 에폭 176 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 177 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 178 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 179 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 180 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 181 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 182 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
      "| 에폭 183 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
      "| 에폭 184 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
      "| 에폭 185 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 186 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 187 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
      "| 에폭 188 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 189 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 190 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 191 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 192 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
      "| 에폭 193 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
      "| 에폭 194 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 195 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 196 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
      "| 에폭 197 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
      "| 에폭 198 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 199 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
      "| 에폭 200 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
      "| 에폭 201 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
      "| 에폭 202 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
      "| 에폭 203 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 204 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
      "| 에폭 205 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 206 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 207 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 208 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 209 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 210 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
      "| 에폭 211 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 212 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 213 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 214 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 215 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 216 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
      "| 에폭 217 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 218 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 219 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 220 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 221 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 222 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 223 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 224 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 225 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 226 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
      "| 에폭 227 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 228 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 229 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 230 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 231 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 232 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 233 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 234 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 235 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 236 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 237 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 238 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 239 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 240 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 241 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 242 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 243 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 244 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 245 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 246 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 247 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 248 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 249 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 250 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 251 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 252 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 253 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 254 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 255 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 256 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 257 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 258 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 259 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 260 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 261 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 262 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 263 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 264 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 265 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 266 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 267 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 268 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 269 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 270 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 271 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 272 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 273 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 274 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 275 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 276 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 277 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 278 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 279 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 280 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 281 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 282 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 283 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 284 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 285 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 286 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 287 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
      "| 에폭 288 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 289 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 290 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 291 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 292 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 293 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 294 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 295 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 296 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 297 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 298 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 299 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 300 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 301 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 302 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 303 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 304 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 305 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 306 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 307 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 308 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 309 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 310 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 311 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 312 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 313 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
      "| 에폭 314 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 315 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 316 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 317 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 318 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 319 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 320 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 321 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 322 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 323 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 324 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 325 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 326 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 327 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 328 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 329 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 330 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 331 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 332 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 333 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 334 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
      "| 에폭 335 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 336 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 337 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 338 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 339 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 340 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 341 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 342 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
      "| 에폭 343 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
      "| 에폭 344 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 345 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 346 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 347 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 348 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 349 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 350 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 351 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 352 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 353 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 354 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 355 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 356 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 357 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 358 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 359 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 360 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 361 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
      "| 에폭 362 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 363 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
      "| 에폭 364 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 365 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 366 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
      "| 에폭 367 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 368 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 369 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 370 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 371 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 372 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
      "| 에폭 373 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 374 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 375 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 376 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 377 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 378 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
      "| 에폭 379 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 380 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 381 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 382 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 383 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 384 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 385 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 386 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 387 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
      "| 에폭 388 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 389 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 390 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 391 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 392 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 393 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 394 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 395 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
      "| 에폭 396 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 397 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 398 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 399 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 400 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 401 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 402 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
      "| 에폭 403 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 404 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 405 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 406 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
      "| 에폭 407 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 408 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 409 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 410 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 411 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 412 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
      "| 에폭 413 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 414 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 415 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
      "| 에폭 416 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 417 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
      "| 에폭 418 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 419 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 420 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 421 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
      "| 에폭 422 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 423 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 424 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 425 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 426 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 427 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 428 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 429 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 430 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 431 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 432 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 433 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 434 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 435 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 436 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 437 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 438 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 439 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 440 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 441 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
      "| 에폭 442 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 443 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 444 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 445 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
      "| 에폭 446 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 447 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 448 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 449 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 450 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 451 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 452 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 453 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
      "| 에폭 454 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 455 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 456 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 457 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 458 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 459 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 460 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 461 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 462 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 463 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 464 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 465 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 466 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 467 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 468 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 469 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 470 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 471 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
      "| 에폭 472 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 473 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 474 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 475 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 476 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 477 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 478 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 479 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 480 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 481 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 482 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
      "| 에폭 483 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 484 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 485 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 486 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
      "| 에폭 487 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
      "| 에폭 488 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 489 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 490 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 491 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 492 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 493 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 494 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
      "| 에폭 495 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 496 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 497 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
      "| 에폭 498 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 499 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 500 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 501 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 502 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 503 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 504 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 505 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 506 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
      "| 에폭 507 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 508 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 509 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 510 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 511 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 512 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 513 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
      "| 에폭 514 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 515 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 516 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 517 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 518 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 519 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 520 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 521 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 522 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 523 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 524 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 525 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 526 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 527 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 528 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 529 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 530 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 531 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 532 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 533 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 534 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 535 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 536 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 537 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
      "| 에폭 538 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 539 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
      "| 에폭 540 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 541 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 542 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
      "| 에폭 543 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 544 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 545 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 546 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 547 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 548 |  반복 1 / 2 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 549 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 550 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 551 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 552 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 553 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 554 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 555 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
      "| 에폭 556 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 557 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 558 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 559 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 560 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 561 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 562 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 563 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 564 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 565 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 566 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 567 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 568 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 569 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 570 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 571 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 572 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
      "| 에폭 573 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 574 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 575 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 576 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 577 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 578 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 579 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 580 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
      "| 에폭 581 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 582 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 583 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 584 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 585 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 586 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
      "| 에폭 587 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 588 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 589 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 590 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 591 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 592 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 593 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 594 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 595 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 596 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 597 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 598 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 599 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 600 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 601 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 602 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 603 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 604 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 605 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 606 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 607 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 608 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 609 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
      "| 에폭 610 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 611 |  반복 1 / 2 | 시간 0[s] | 손실 0.29\n",
      "| 에폭 612 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 613 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 614 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 615 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 616 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 617 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 618 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 619 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 620 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 621 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 622 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
      "| 에폭 623 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 624 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 625 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 626 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
      "| 에폭 627 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 628 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 629 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 630 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 631 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 632 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 633 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 634 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 635 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 636 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 637 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 638 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 639 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 640 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 641 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 642 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 643 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 644 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 645 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 646 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 647 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 648 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
      "| 에폭 649 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 650 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 651 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 652 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 653 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 654 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 655 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 656 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 657 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 658 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 659 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 660 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 661 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 662 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 663 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 664 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 665 |  반복 1 / 2 | 시간 0[s] | 손실 0.29\n",
      "| 에폭 666 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 667 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 668 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 669 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 670 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 671 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 672 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 673 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 674 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 675 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 676 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 677 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 678 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 679 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 680 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 681 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 682 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 683 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 684 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 685 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 686 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 687 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 688 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 689 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 690 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 691 |  반복 1 / 2 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 692 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 693 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 694 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 695 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
      "| 에폭 696 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 697 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 698 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 699 |  반복 1 / 2 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 700 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 701 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 702 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 703 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 704 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 705 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 706 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 707 |  반복 1 / 2 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 708 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 709 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 710 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 711 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 712 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 713 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 714 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 715 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 716 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 717 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 718 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 719 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 720 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 721 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 722 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 723 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 724 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 725 |  반복 1 / 2 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 726 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 727 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 728 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 729 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 730 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 731 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 732 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 733 |  반복 1 / 2 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 734 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 735 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 736 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 737 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 738 |  반복 1 / 2 | 시간 0[s] | 손실 0.29\n",
      "| 에폭 739 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 740 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
      "| 에폭 741 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 742 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 743 |  반복 1 / 2 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 744 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 745 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 746 |  반복 1 / 2 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 747 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 748 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 749 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 750 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 751 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 752 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 753 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 754 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 755 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 756 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 757 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 758 |  반복 1 / 2 | 시간 0[s] | 손실 0.29\n",
      "| 에폭 759 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 760 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 761 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 762 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 763 |  반복 1 / 2 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 764 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 765 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
      "| 에폭 766 |  반복 1 / 2 | 시간 0[s] | 손실 0.29\n",
      "| 에폭 767 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 768 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 769 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 770 |  반복 1 / 2 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 771 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 772 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 773 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 774 |  반복 1 / 2 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 775 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 776 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 777 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 778 |  반복 1 / 2 | 시간 0[s] | 손실 0.29\n",
      "| 에폭 779 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 780 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 781 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 782 |  반복 1 / 2 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 783 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 784 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 785 |  반복 1 / 2 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 786 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
      "| 에폭 787 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 788 |  반복 1 / 2 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 789 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 790 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 791 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 792 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 793 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 794 |  반복 1 / 2 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 795 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 796 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 797 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 798 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 799 |  반복 1 / 2 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 800 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 801 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 802 |  반복 1 / 2 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 803 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 804 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 805 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 806 |  반복 1 / 2 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 807 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 808 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 809 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 810 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
      "| 에폭 811 |  반복 1 / 2 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 812 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 813 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 814 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 815 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 816 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 817 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 818 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 819 |  반복 1 / 2 | 시간 0[s] | 손실 0.15\n",
      "| 에폭 820 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 821 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
      "| 에폭 822 |  반복 1 / 2 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 823 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 824 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 825 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 826 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 827 |  반복 1 / 2 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 828 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 829 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 830 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 831 |  반복 1 / 2 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 832 |  반복 1 / 2 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 833 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 834 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 835 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 836 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 837 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 838 |  반복 1 / 2 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 839 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 840 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
      "| 에폭 841 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 842 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
      "| 에폭 843 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 844 |  반복 1 / 2 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 845 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
      "| 에폭 846 |  반복 1 / 2 | 시간 0[s] | 손실 0.26\n",
      "| 에폭 847 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 848 |  반복 1 / 2 | 시간 0[s] | 손실 0.28\n",
      "| 에폭 849 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 850 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 851 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 852 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 853 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 854 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 855 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 856 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 857 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
      "| 에폭 858 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 859 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 860 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 861 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 862 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 863 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 864 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 865 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
      "| 에폭 866 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 867 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 868 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 869 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 870 |  반복 1 / 2 | 시간 0[s] | 손실 0.16\n",
      "| 에폭 871 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 872 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 873 |  반복 1 / 2 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 874 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
      "| 에폭 875 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 876 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 877 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 878 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 879 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 880 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 881 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 882 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 883 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 884 |  반복 1 / 2 | 시간 0[s] | 손실 0.27\n",
      "| 에폭 885 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 886 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 887 |  반복 1 / 2 | 시간 0[s] | 손실 0.14\n",
      "| 에폭 888 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 889 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 890 |  반복 1 / 2 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 891 |  반복 1 / 2 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 892 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 893 |  반복 1 / 2 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 894 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 895 |  반복 1 / 2 | 시간 0[s] | 손실 0.37\n",
      "| 에폭 896 |  반복 1 / 2 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 897 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
      "| 에폭 898 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 899 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 900 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 901 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 902 |  반복 1 / 2 | 시간 0[s] | 손실 0.25\n",
      "| 에폭 903 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 904 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 905 |  반복 1 / 2 | 시간 0[s] | 손실 0.10\n",
      "| 에폭 906 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 907 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 908 |  반복 1 / 2 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 909 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 910 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 911 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 912 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 913 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 914 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 915 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 916 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 917 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 918 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 919 |  반복 1 / 2 | 시간 0[s] | 손실 0.09\n",
      "| 에폭 920 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
      "| 에폭 921 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 922 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 923 |  반복 1 / 2 | 시간 0[s] | 손실 0.36\n",
      "| 에폭 924 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 925 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 926 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 927 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 928 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 929 |  반복 1 / 2 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 930 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 931 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
      "| 에폭 932 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 933 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 934 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 935 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 936 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 937 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 938 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 939 |  반복 1 / 2 | 시간 0[s] | 손실 0.11\n",
      "| 에폭 940 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
      "| 에폭 941 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 942 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 943 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 944 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 945 |  반복 1 / 2 | 시간 0[s] | 손실 0.24\n",
      "| 에폭 946 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 947 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 948 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 949 |  반복 1 / 2 | 시간 0[s] | 손실 0.13\n",
      "| 에폭 950 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 951 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 952 |  반복 1 / 2 | 시간 0[s] | 손실 0.29\n",
      "| 에폭 953 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 954 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
      "| 에폭 955 |  반복 1 / 2 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 956 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 957 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 958 |  반복 1 / 2 | 시간 0[s] | 손실 0.29\n",
      "| 에폭 959 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 960 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 961 |  반복 1 / 2 | 시간 0[s] | 손실 0.29\n",
      "| 에폭 962 |  반복 1 / 2 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 963 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 964 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 965 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 966 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 967 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 968 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 969 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 970 |  반복 1 / 2 | 시간 0[s] | 손실 0.23\n",
      "| 에폭 971 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 972 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 973 |  반복 1 / 2 | 시간 0[s] | 손실 0.19\n",
      "| 에폭 974 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 975 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
      "| 에폭 976 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 977 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 978 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 979 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
      "| 에폭 980 |  반복 1 / 2 | 시간 0[s] | 손실 0.22\n",
      "| 에폭 981 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 982 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 983 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 984 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 985 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 986 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 987 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
      "| 에폭 988 |  반복 1 / 2 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 989 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
      "| 에폭 990 |  반복 1 / 2 | 시간 0[s] | 손실 0.21\n",
      "| 에폭 991 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
      "| 에폭 992 |  반복 1 / 2 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 993 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 994 |  반복 1 / 2 | 시간 0[s] | 손실 0.32\n",
      "| 에폭 995 |  반복 1 / 2 | 시간 0[s] | 손실 0.20\n",
      "| 에폭 996 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
      "| 에폭 997 |  반복 1 / 2 | 시간 0[s] | 손실 0.31\n",
      "| 에폭 998 |  반복 1 / 2 | 시간 0[s] | 손실 0.33\n",
      "| 에폭 999 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n",
      "| 에폭 1000 |  반복 1 / 2 | 시간 0[s] | 손실 0.30\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f09bc86b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ch03/train.py\n",
    "from matplotlib import font_manager, rc, rcParams\n",
    "rcParams['axes.unicode_minus'] = False\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from simple_cbow import SimpleCBOW\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 3  # 변경 조절! (위의 설명에서는 3으로 가정)\n",
    "batch_size = 3   #\n",
    "max_epoch = 1000\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습이 끝난 후의 가중치 매개변수를 살펴보자\n",
    "\n",
    "- 확인 방법 : 입력 측 MatMul 계층의 가중치는 인스턴스 변수 **`word_vecs`**에 저장되어 있으니, 앞의 코드 바로 뒤에 다음 코드를 추가해보자.\n",
    "\n",
    "- $W_{in}$ : 형상(shape)는 ? 말뭉치 내 단어가 7개이며, hidden_size = 5를 상기시키자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [ 0.873115    1.6785808   0.8641359   0.9246482  -0.95318377]\n",
      "say [-1.2075272   0.42709675 -1.229767   -1.1949221   0.2623294 ]\n",
      "goodbye [ 0.9939556  -0.46733648  1.0177968   1.0430483  -1.0878863 ]\n",
      "and [-0.8770509   1.5462736  -0.9093968  -0.91993135  1.4974284 ]\n",
      "i [ 1.0182855  -0.47983995  1.0556281   1.0798137  -1.1411686 ]\n",
      "hello [ 0.8480036   1.6845031   0.83802855  0.92427194 -0.9223161 ]\n",
      ". [-1.1298842 -1.3999565 -1.1625289 -1.1126215 -1.6187906]\n"
     ]
    }
   ],
   "source": [
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 단어마다 밀집벡터로 나타낼 수 있게 되었다. 이 단어에 해당하는 벡터 하나가 바로 단어의 **분산 표현**이다.\n",
    "\n",
    "하지만, 아쉽게도 여기서 다룬 작은 말뭉치로는 좋은 결과를 얻을 수 없었다. 이유 : 데이터가 작기 때문에.. 또한 실용적이고 충분히 큰 말뭉치로 바꾸면 좋은 결과도 얻을 수 있지만, 처리 속도면에서 문제점이 발생한다. \n",
    "\n",
    "지금까지 구현한 **CBOW** 모델은 처리 효율 관점에서 문제가 있다. 그래서 다음 Ch4 에서는 현재의 \"단순한\" CBOW 모델을 개선하여 \"진짜\" CBOW 모델을 구현할 예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 word2vec 보충\n",
    "\n",
    "- **word2vec**에 관한 중요한 주제 보충\n",
    "    1. **CBOW** 모델을 \"확률적\" 관점에서 다시 살펴보자.\n",
    "    2. **skip-gram** 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 CBOW 모델과 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률\n",
    "$$\n",
    "P(A) : \\text{A라는 현상이 일어날 확률}\n",
    "$$\n",
    "\n",
    "- 동시 확률\n",
    "$$\n",
    "P(A, B) : \\text{A와 B가 동시에 일어날 확률}\n",
    "$$\n",
    "\n",
    "- 사후 확률 : \"**사건**이 일어난 **후**의 **확률**\"\n",
    "$$\n",
    "P(A | B) : \\text{B(라는 정보)가 주어졌을 때 A가 일어날 확률}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CBOW** 모델을 확률 표기법으로 기술해 보자.\n",
    "\n",
    "**[그림 3-24]** COBW 모델의 신경망 구성 예\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-22.PNG?raw=true\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 맥락으로 $Word_{t-1}$ 과 $Word_{t+1}$ 이 주어졌을 때, $Word_{t}$가 될 확률은 수식으로 다음과 같이 나타낼 수 있다.\n",
    "\n",
    "- **CBOW 모델을 확률로 표기** \n",
    "\n",
    "$$\n",
    "P(Word_{t} | Word_{t-1}, Word_{t+1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, **CBOW**는 위와 같은 식을 모델링하고 있는 것이다.  \n",
    "위의 식을 이용해서 **CBOW** 모델의 손실 함수도 표현할 수 있다.\n",
    "\n",
    "음의 로그 가능도 **negative log likelihood** 를 이용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 손실 함수(샘플 데이터 하나)\n",
    "\n",
    "$$\n",
    "L = - \\log P(Word_{t}|Word_{t-1} , Word_{t+1})\n",
    "$$\n",
    "\n",
    "- 손실 함수(말뭉치 전체로 확장 : T)\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{T} \\sum_{t=1}^{T}  \\log P(Word_{t}|Word_{t-1} , Word_{t+1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 $L$를 가능한 한 작게 만드는 것이 우리의 목적, 그리고 이때의 가중치 매개변수가 우리가 얻고자 하는 단어의 분산 표현이다. 여기에서 윈도우 크기가 1인 경우만 생각했지만, 다른 크기(또는 $m$ 등의 범용적인 크기)라 해도 수식으로 쉽게 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 skip-gram 모델\n",
    "\n",
    "- word2vec은 skip-gram 모델과 CBOW 모델을 제공\n",
    "\n",
    "**skip-gram** 모델은 **CBOW** 모델에서 다루는 맥락과 타깃을 역전시킨 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[그림 3-23]** CBOW 모델과 skip-gram 모델이 다루는 문제\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-23.PNG?raw=true\" width=\"450\">\n",
    "\n",
    "\n",
    "**[그림 3-24]** skip-gram 모델의 신경망 구성\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/DeepLearningFromScratch_II/blob/master/PNG/Figure%203-24.PNG?raw=true\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram 모델을 확률 표기로 나타내 보자.\n",
    "\n",
    "- **skip-gram 모델을 확률 표기**\n",
    "\n",
    "$$\n",
    "P(Word_{t-1}, Word_{t+1} | Word_{t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **맥락의 단어들 사이에 관련성이 없다고 가정하고 다음과 같이 분해 가능하다.**\n",
    "\n",
    "\n",
    "$$\n",
    "P(Word_{t-1}, Word_{t+1} | Word_{t}) = P(Word_{t-1}|Word_{t}) \\cdot P(Word_{t+1}|Word_{t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **손실 함수(샘플 데이터 하나)** : 교차 엔트로피 오차에 적용하여 손실 함수 유도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\n",
    "L &= - \\log P(Word_{t-1} , Word_{t+1} | Word_{t}) \\\\\n",
    "  &= - \\log P(Word_{t-1}|Word_{t}) \\cdot P(Word_{t+1}|Word_{t}) \\\\\n",
    "  &= - (\\log P(Word_{t-1}|Word_{t}) + P(Word_{t+1}|Word_{t})) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "- **손실 함수**(말뭉치 전체로 확장 : T)\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{T} \\sum_{t=1}^{T} (\\log P(Word_{t-1}|Word_{t}) + P(Word_{t+1}|Word_{t}))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**skip-gram** 모델은 맥락의 수만큼 추측하기 때문에 손실 함수는 각 맥락에서 구한 손실의 총합이다. 반면, **CBOW** 모델은 타깃 하나의 손실을 구한다. \n",
    "\n",
    "**질문**\n",
    "\n",
    "- **COBW** 모델과 **skip-gram** 모델 중 어느 것을 사용해야 할까? \n",
    "    - 답 : **skip-gram** 모델\n",
    "    - 이유 : 단어 분산 표현의 정밀도면에서 모델의 결과가 더 좋은 경우가 많다. 특히 말뭉치가 커질수록 저빈도 단어나 유추 문제의 성능 면에서 skip-gram 모델이 떠 뛰어난 경향이 있다.(단어 분산 표현의 성능 : 다음주 Ch4. 단어 벡터 평가 방법 절 참고..),\n",
    "\n",
    "> **Note**  \n",
    "> \"더 어려운\" 문제를 도전한다. 그리고 더 어려운 상황에서 단련하는 만큼 **skip-model** 모델이 내어 주는 단어의 분산 표현이 더 뛰어날 가능성이 커진다.\n",
    "\n",
    "> 추가 답변 : 갱신하는 관점에서 바로보기!\n",
    "\n",
    "- 학습 속도면에서는 **CBOW** 모델이 더 빠를 것.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 통계 기반 vs 추론 기반"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**어휘에 추가할 새 단어가 생겨서 단어의 분산 표현을 갱신해야 하는 상황**인 경우 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 통계 기반 기법\n",
    "    - 처음부터 다시 계산\n",
    "    \n",
    "- 추론 기반 기법\n",
    "    - 매개변수를 다시 학습(부분적)\n",
    "    - 지금까지 학습한 가중치를 초깃값으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 기법으로 얻는 단어의 분산 표현의 성격이나 정밀도 면에서는 어떤 차이가 있을까?\n",
    "- 통계 기반 기법\n",
    "    - 분산 표현 관점 : 단어의 유사성이 인코딩됨\n",
    "    \n",
    "- 추론 기반 기법\n",
    "    - 분산 표현 관점 : 단어의 유사성 + 복잡한 단어 사이의 패턴 (예 : king - man + woman = queen\")\n",
    "    \n",
    " 위의 차이로 많은 분들이 추론 기반 기법이 통계 기반 기법보다 정확하다고 흔히들 오해하지만, 우열을 가릴 수 없다고 함.\n",
    " \n",
    "#### 더 중요한 사실은 추론 기반 기법과 통계 기반 기법은 서로 관련되어 있다고 한다.  \n",
    "이유 : skip-gram 모델과 네거티브 샘플링을 이용한 모델은 모두 말뭉치 전체의 동시발생 행렬(실제로는 살짝 수정한 행렬)에 특수한 행렬 분해를 적용한 것과 같다고 한다. 두 세계는 (특정 조건 하에서) \"서로 연결되어 있다.\"고 할 수 있다.\n",
    "\n",
    "나아가 word2vec 이후 추론 기반 기법과 통계 기반 기법을 융합한 \"Glove\" 기법이 등장함.\n",
    "- 말뭉치 전체의 통계 정보를 손실 함수에 도입해 미니배치 학습을 하는 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
